{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159cb80-786d-4f22-b40f-fe2f5851ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import argparse\n",
    "\n",
    "from ddpm import *\n",
    "from data_make import *\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a283bfa8-56f9-4d7a-b509-8b96f990e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load and preprocess data\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "class Sine_Pytorch(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, no_samples, seq_len, features):\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        for i in range(no_samples):\n",
    "            \n",
    "            temp = []\n",
    "            \n",
    "            for k in range(features):\n",
    "                \n",
    "                freq = np.random.uniform(0, 0.1)\n",
    "                \n",
    "                phase = np.random.uniform(0, 0.1)\n",
    "                \n",
    "                temp_data = [np.sin(freq*j + phase) for j in range(seq_len)]\n",
    "                \n",
    "                temp.append(temp_data)\n",
    "                \n",
    "            temp = np.transpose(np.asarray(temp))\n",
    "            \n",
    "            temp = (temp + 1) * 0.5\n",
    "            \n",
    "            self.data.append(temp)\n",
    "        \n",
    "        self.data = np.asarray(self.data, dtype = np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.data[idx, :, :]\n",
    "#################################################    \n",
    "\n",
    "def data_preprocess(dataset_name):\n",
    "    \n",
    "    data_dir = f'data'\n",
    "    \n",
    "    if dataset_name == 'air':\n",
    "        \n",
    "        data = pd.read_csv(f'{data_dir}/AirQualityUCI.csv', delimiter= ';', decimal = ',')\n",
    "        \n",
    "        # Last 114 rows does not contain any values\n",
    "        \n",
    "        data = data.iloc[:-114, 2:15]\n",
    "        \n",
    "    elif dataset_name == 'energy':\n",
    "        \n",
    "        data = pd.read_csv(f'{data_dir}/energydata_complete.csv')\n",
    "        \n",
    "        data = data.iloc[:, 1:]\n",
    "        \n",
    "    elif dataset_name == 'stock':\n",
    "        \n",
    "        #data = pd.read_csv(f'{data_dir}/GOOG.csv')\n",
    "        #print(\"data_preprocess data pre\", data)\n",
    "        #data = data.iloc[:, 1:]\n",
    "        #print(\"data_preprocess data after\", data)\n",
    "        #print(data.shape)\n",
    "\n",
    "        data_path = r\"/mnt/c/Users/hugom/OneDrive/Documentos/ETHZ academic/Project/Code/Data_extraction/data500SP_v1.csv\"\n",
    "        data1 = pd.read_csv(data_path, index_col=0, header=[0,1])\n",
    "        data1.index = pd.to_datetime(data1.index)\n",
    "        \n",
    "        # Stocks to be excluded from analysis\n",
    "        #incomplete_stocks = [\"FOXA\",\"FOX\",\"DOW\",\"UBER\",\"CTVA\",\"OTIS\",\"CARR\",\"ABNB\",\"CEG\",\"GEHC\",\"KVUE\",\"VLTO\"]\n",
    "        # incomplete_stocks = [\"FOXA\",\"FOX\",\"DOW\",\"UBER\",\"CTVA\",\"OTIS\",\"CARR\",\"ABNB\",\"CEG\",\"GEHC\",\"KVUE\",\"VLTO\",\"APA\",\"TRGP\",\"OXY\",\"PCG\"]\n",
    "        # Computation of log of returns\n",
    "        log_returns = data1['Adj Close'].div(data1['Adj Close'].shift(periods=1)).apply(np.log)[1:]\n",
    "        log_returns = log_returns[[\"GOOG\"]]\n",
    "        #print(log_returns.shape)\n",
    "        data = log_returns\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "class MakeDATA(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        \n",
    "        data = np.asarray(data, dtype= np.float32)\n",
    "        \n",
    "        data = data[::-1]\n",
    "        print(\"MakeDATA data\", data.shape)\n",
    "\n",
    "        norm_data = normalize(data)\n",
    "        #norm_data = data\n",
    "\n",
    "        seq_data = []\n",
    "        for i in range(len(norm_data) - seq_len + 1):\n",
    "            x = norm_data[i : i + seq_len]\n",
    "            seq_data.append(x)\n",
    "\n",
    "        self.samples = []\n",
    "        idx = torch.randperm(len(seq_data))\n",
    "        for i in range(len(seq_data)):\n",
    "            self.samples.append(seq_data[idx[i]])\n",
    "            \n",
    "        self.samples = np.asarray(self.samples, dtype = np.float32)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    \n",
    "    \n",
    "def LoadData(dataset_name, seq_len):\n",
    "    \n",
    "    if dataset_name == 'sine':\n",
    "        \n",
    "        data = Sine_Pytorch(5000, seq_len, 5)\n",
    "        \n",
    "        train_data, test_data = train_test_split(data, train_size = 0.8, random_state = 2021)\n",
    "        \n",
    "        print(f'Sine data loaded with sequence {seq_len}')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        data = data_preprocess(dataset_name)\n",
    "        data = MakeDATA(data, seq_len)\n",
    "        print(\"LoadData MakeDATA\", data)\n",
    "        train_data, test_data = train_test_split(data, train_size = 0.8, random_state = 2021)\n",
    "        \n",
    "        print(f'{dataset_name} data loaded with sequence {seq_len}')\n",
    "        \n",
    "    return train_data, test_data\n",
    "\n",
    "#train_data, test_data = LoadData(dataset_name, seq_len)\n",
    "#print(len(train_data))\n",
    "#print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78aca4-41d5-466c-b2df-25b47c8916f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "data_path = r\"/mnt/c/Users/hugom/OneDrive/Documentos/ETHZ academic/Project/Code/Data_extraction/data500SP_v1.csv\"\n",
    "data1 = pd.read_csv(data_path, index_col=0, header=[0,1])\n",
    "data1.index = pd.to_datetime(data1.index)\n",
    "\n",
    "# Computation of log of returns\n",
    "#log_returns = data1['Adj Close'].div(data1['Adj Close'].shift(periods=1)).apply(np.log)[1:]\n",
    "log_returns = data1['Adj Close']\n",
    "log_returns = log_returns[[\"GOOG\"]]\n",
    "#print(log_returns.shape)\n",
    "#data_raw = log_returns.values\n",
    "data_raw = log_returns\n",
    "\n",
    "data_pre = data_raw\n",
    "print(data_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f817d4-c449-48ef-9c5d-33198931d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model arguments\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--dataset_name',\n",
    "    choices=['sine','stock','air', 'energy'],\n",
    "    default='stock', ##\n",
    "    type=str)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--beta_schedule',\n",
    "    choices=['cosine','linear', 'quadratic', 'sigmoid'],\n",
    "    default='cosine',\n",
    "    type=str)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--objective',\n",
    "    choices=['pred_x0','pred_v', 'pred_noise'],\n",
    "    default='pred_v',\n",
    "    type=str)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--seq_len',\n",
    "    help='sequence length',\n",
    "    default=1256, ###\n",
    "    type=int)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--batch_size',\n",
    "    help='batch size for the network',\n",
    "    default=256,\n",
    "    type=int)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--n_head',\n",
    "    help='number of heads for the attention',\n",
    "    default=8,##\n",
    "    type=int)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--hidden_dim',\n",
    "    help='number of hidden state',\n",
    "    default=256, ##\n",
    "    type=int)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--num_of_layers',\n",
    "    help='Number of Layers',\n",
    "    default=6, ##\n",
    "    type=int)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--training_epoch',\n",
    "    help='Diffusion Training Epoch',\n",
    "    default=3000, ##\n",
    "    type=int)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--timesteps',\n",
    "    help='Timesteps for Diffusion',\n",
    "    default=1000,\n",
    "    type=int)\n",
    "\n",
    "args = parser.parse_args('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65304b-e406-4ff5-a5f9-fbe5a56f7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting model parameters\n",
    "\n",
    "seq_len = args.seq_len\n",
    "epochs = args.training_epoch\n",
    "timesteps = args.timesteps\n",
    "batch_size = args.batch_size\n",
    "latent_dim = args.hidden_dim\n",
    "num_layers = args.num_of_layers\n",
    "n_heads = args.n_head    \n",
    "dataset_name = args.dataset_name\n",
    "beta_schedule = args.beta_schedule\n",
    "objective = args.objective\n",
    "\n",
    "#train_data, test_data = LoadData(dataset_name, seq_len)\n",
    "data_ready = MakeDATA(data_pre, seq_len)\n",
    "train_data, test_data = train_test_split(data_ready, train_size = 0.8, random_state = 0)\n",
    "\n",
    "\n",
    "train_data, test_data = np.asarray(train_data), np.asarray(test_data)\n",
    "\n",
    "features = train_data.shape[2]\n",
    "\n",
    "train_data, test_data = train_data.transpose(0,2,1), test_data.transpose(0,2,1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, len(test_data))\n",
    "\n",
    "real_data = next(iter(test_loader))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "mode = 'diffusion'\n",
    "\n",
    "architecture = 'custom-transformers'\n",
    "\n",
    "loss_mode = 'l1'\n",
    "\n",
    "file_name = f'{architecture}-{dataset_name}-{loss_mode}-{beta_schedule}-{seq_len}-{objective}'\n",
    "\n",
    "folder_name = f'saved_files/{time.time():.4f}-{file_name}'\n",
    "\n",
    "pathlib.Path(folder_name).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "gan_fig_dir_path = f'{folder_name}/output/gan'\n",
    "\n",
    "pathlib.Path(gan_fig_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_name_gan_fig = f'{file_name}-gan'\n",
    "\n",
    "with open(f'{folder_name}/params.txt', 'w') as f:\n",
    "    \n",
    "    json.dump(args.__dict__, f, indent=2)\n",
    "    \n",
    "    f.close() \n",
    "\n",
    "writer = SummaryWriter(log_dir = folder_name, comment = f'{file_name}', flush_secs = 45)\n",
    "\n",
    "\n",
    "model = TransEncoder(\n",
    "\n",
    "    features = features,\n",
    "    latent_dim = latent_dim,\n",
    "    num_heads = n_heads,\n",
    "    num_layers = num_layers\n",
    "\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion1D(\n",
    "    model,\n",
    "    seq_length = seq_len,\n",
    "    timesteps = timesteps,  \n",
    "    objective = objective, # pred_x0, pred_v\n",
    "    loss_type = 'l2',\n",
    "    beta_schedule = beta_schedule\n",
    ")\n",
    "\n",
    "diffusion = diffusion.to(device)\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "betas = (0.9, 0.99)\n",
    "\n",
    "optim = torch.optim.Adam(diffusion.parameters(), lr = lr, betas = betas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99904b7c-8baf-47e1-8c75-2346e6f1a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to verify if it is using the desired device nad number of dimensions of the time series\n",
    "print(device)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a032703-1110-4d3e-83a1-1b4d50bbdb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training stage of the model\n",
    "for running_epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        data = data.to(device)\n",
    "        \n",
    "        batch_size = data.shape[0]\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        loss = diffusion(data)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        \n",
    "        if i%len(train_loader)==0:\n",
    "            \n",
    "            writer.add_scalar('Loss', loss.item(), running_epoch)\n",
    "            \n",
    "        if i%len(train_loader)==0 and running_epoch%100==0:\n",
    "            \n",
    "            print(f'Epoch: {running_epoch+1}, Loss: {loss.item()}')\n",
    "            \n",
    "        if i%len(train_loader)==0 and running_epoch%500==0:\n",
    "            print(f'Epoch: {running_epoch+1}, Loss: {loss.item()}')\n",
    "            #with torch.no_grad():\n",
    "                \n",
    "                #samples = diffusion.sample(len(test_data))\n",
    "\n",
    "                #samples = samples.cpu().numpy()\n",
    "\n",
    "                #samples = samples.transpose(0, 2, 1)\n",
    "                \n",
    "                #np.save(f'{folder_name}/synth-{dataset_name}-{seq_len}-{running_epoch}.npy', samples)\n",
    "                \n",
    "            # visualize(real_data.cpu().numpy().transpose(0,2,1), samples, dataset_name, seq_len, gan_fig_dir_path, running_epoch, writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e98db-b03a-4688-93b7-0af5e0a3dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "'epoch': running_epoch+1,\n",
    "'diffusion_state_dict': diffusion.state_dict(),\n",
    "'diffusion_optim_state_dict': optim.state_dict()\n",
    "}, os.path.join(f'{folder_name}', f'{file_name}-final.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa77004-b526-4f83-822e-c7831b2c07b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of artificial paths (time series)\n",
    "M = 10\n",
    "list_traj = []\n",
    "for m in range(M):\n",
    "    set_seed(m)\n",
    "    x_fake_m = diffusion.sample(len(test_data))\n",
    "    print(x_fake_m.shape)\n",
    "    x_fake_m = x_fake_m.cpu().numpy()\n",
    "    print(x_fake_m.shape)\n",
    "    x_fake_m = x_fake_m.transpose(0, 2, 1)\n",
    "    print(x_fake_m.shape)\n",
    "    list_traj.append(x_fake_m)\n",
    "\n",
    "# x_fake_M = torch.cat(list_traj, 0)\n",
    "x_fake_M = np.concatenate(list_traj, 0)\n",
    "\n",
    "print(\"x_fake_M.shape\", x_fake_M.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7febd042-8244-48ee-91b4-d5fef890d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fake_M[0:2,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb5c67-b8e5-4ef4-b8b7-f15e29de8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing size of plot\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "traj = x_fake_M[0,:1100,0]\n",
    "ts = np.log(traj[1:]) - np.log(traj[:-1])\n",
    "plt.plot(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b35d16-dec0-4820-aee7-59554567c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73579740-6773-4860-b0ef-3c7ce31a86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ts).T.to_csv(\"samples_fake.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
